{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"roc.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pdp.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This we dove into tree-base methods, started learning Natural Langugage Processing, and learned some clustering techniques as well. We had our second case study of Friday and it went a lot smoother than the previous one last week. Not only did we have more knowledge, but we also had better understandings of how to work in a one-day time frame, collaborative coding with Git and Github, and approriate file architecture to this type of project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a rundown of what we covered:\n",
    "\n",
    "* Decision trees\n",
    " * Bagged trees\n",
    "* Random forests\n",
    " * Bagging plus random feature sub-setting\n",
    "* Boosting\n",
    " * Adaboost\n",
    " * Gradient boosting\n",
    " * XGBoost\n",
    " * Catboost\n",
    "* Natural Language Processing\n",
    " * Word to vector\n",
    " * Bag of words\n",
    "* Naive Bayes classification\n",
    "* KMeans clustering\n",
    "* Hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree method breakdown:\n",
    "\n",
    "Algorithm | Learner Type | Hyperparameters | Pros | Cons\n",
    "--- | --- | --- | --- | ---\n",
    "Decision Trees | Strong | Impurty Function | No scaling, model non-linear relationships, classification and regression, predictions cheap, highly interpretable | expensive to train, needs pruning to avoid overfitting\n",
    "Random Forests | Strong | Number of trees, number of features at each split, individual tree parameters, tree depth, pruning, split criterion | great performance, often little tuning needed, no feature scaling, model non-linear relationships | expensive to train, hard to interpret effect of each feature\n",
    "Boosted Trees | weak | number of estimators, learning rate alpha, max depth, max features, subsample amount | squeezes out predictive power, resistant to overfiting, smoother that RF | can't be trained in paralell like RF, could cause overfitting with too many trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some code we wrote:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    \"\"\"Calculate the accuracy score\n",
    "    \"\"\"\n",
    "    return np.mean(y_hat == y)\n",
    "\n",
    "def recall(y_hat, y):\n",
    "    ''' Calculate recall score'''\n",
    "    return np.mean()\n",
    "    pass\n",
    "\n",
    "'''\n",
    "This script has functions to clean the test and train data\n",
    "as well as engineer new features, for use in the plots and churn_model\n",
    "scripts\n",
    "'''\n",
    "class DataCleaning(BaseEstimator, TransformerMixin):\n",
    "#     def get_params(self, **kwargs):\n",
    "#         pass\n",
    "        \n",
    "    def fit(self, df, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        '''\n",
    "        INPUT: UNCLEANED PANDAS DF with target label\n",
    "        OUTPUT: CLEANED PANDAS DF with null value\n",
    "        '''\n",
    "\n",
    "        # convert to datetime\n",
    "        df['last_trip_date'] = pd.to_datetime(df['last_trip_date'])\n",
    "        df['signup_date'] = pd.to_datetime(df['signup_date'])\n",
    "        # convert to 1/0\n",
    "        df['luxury_car_user'] = df['luxury_car_user'].astype(int)\n",
    "\n",
    "        # add missing value to phone device\n",
    "        df['phone'].fillna(value='missing', inplace=True)\n",
    "\n",
    "        # Filling missing values for avg_rating_of_driver\n",
    "        df['avg_rating_of_driver'].fillna(-1,inplace = True)\n",
    "\n",
    "        # Filling missing values for avg_rating_by_driver\n",
    "        df['avg_rating_by_driver'].fillna(-1,inplace = True)\n",
    "\n",
    "        # Create new colums indicating ratings or non-ratings\n",
    "        condition_1 = df['avg_rating_of_driver'] == -1 \n",
    "        df['rating_of_driver'] = 0\n",
    "        df.loc[~condition_1, 'rating_of_driver'] = 0\n",
    "        condition_2 = df['avg_rating_by_driver'] == -1 \n",
    "        df['rating_by_driver'] = 0\n",
    "        df.loc[~condition_2, 'rating_by_driver'] = 0\n",
    "        \n",
    "        cols_to_be_kept = ['avg_dist', 'avg_rating_by_driver', 'rating_by_driver',\\\n",
    "                           'avg_rating_of_driver', 'rating_of_driver',\\\n",
    "                           'avg_surge','city', 'phone', 'surge_pct','trips_in_first_30_days',\\\n",
    "                           'luxury_car_user', 'weekday_pct']\n",
    "        X = df[cols_to_be_kept]\n",
    "\n",
    "        cat_cols = ['phone', 'city']\n",
    "        for col in cat_cols:\n",
    "            df[col] = df[col].astype('category')\n",
    "        X = pd.get_dummies(X, columns=cat_cols)\n",
    "        return X\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def feature_engineer(data):\n",
    "\n",
    "    y = data['churn']\n",
    "    data.drop(['churn'],axis=1,inplace=True)\n",
    "\n",
    "    cols_to_be_kept = ['avg_dist', 'avg_rating_by_driver', 'rating_by_driver',\\\n",
    "                       'avg_rating_of_driver', 'rating_of_driver',\\\n",
    "                       'avg_surge','city', 'phone', 'surge_pct','trips_in_first_30_days',\\\n",
    "                       'luxury_car_user', 'weekday_pct']\n",
    "    X = data[cols_to_be_kept]\n",
    "\n",
    "    cat_cols = ['phone', 'city']\n",
    "    for col in cat_cols:\n",
    "        data[col] = data[col].astype('category')\n",
    "    X = pd.get_dummies(X, columns=cat_cols)\n",
    "    \n",
    "    return X, y\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\" This solution makes heavy use of sklearn's Pipeline class.\n",
    "    You can find documentation on using this class here:\n",
    "    http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\"\"\"\n",
    "from datetime import timedelta\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import PredefinedSplit, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from data_cleaning import DataCleaning\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    \"\"\"Calculate the accuracy score\n",
    "    \"\"\"\n",
    "    return np.mean(y_hat == y)\n",
    "\n",
    "def recall(y_hat, y):\n",
    "    ''' Calculate recall score'''  \n",
    "    return np.mean()\n",
    "    pass\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    df = pd.read_csv('data/churn_train.csv')\n",
    "    # creating dependent churn variables\n",
    "    # labelled customers churned if they hadn't used the service in the last\n",
    "    # month\n",
    "\n",
    "    condition = df['last_trip_date'] < '2014-06-01' \n",
    "    df['churn'] = 1\n",
    "    df.loc[~condition, 'churn'] = 0\n",
    "    y = df['churn']\n",
    "    clean = DataCleaning()\n",
    "    df = clean.transform(df)\n",
    "    \n",
    "    #p = Pipeline([\n",
    "    #    ('dc', DataCleaning()),\n",
    "    #    ('rf', RandomForestClassifier())\n",
    "    #])\n",
    "    \n",
    "    # GridSearch for RF\n",
    "    params = {'n_estimators': [100, 200, 500],\n",
    "             'max_depth': [3, 5, 7],\n",
    "             'max_features': ['auto', 'sqrt', 'log2']}\n",
    "\n",
    "    gb_params = {'learning_rate': [1],\n",
    "                'n_estimators' : [100],\n",
    "                'subsample' : [1],\n",
    "                'max_depth' : [3],\n",
    "                'max_features' : ['auto']\n",
    "    \n",
    "    }\n",
    "\n",
    "    log_params = { 'C' : [1, 2, 3, 4, 5]\n",
    "                \n",
    "    }\n",
    "\n",
    "    rf = RandomForestClassifier()\n",
    "    gb = GradientBoostingClassifier()\n",
    "    lr = LogisticRegression()\n",
    "\n",
    "    acc_scorer = make_scorer(accuracy)\n",
    "\n",
    "    # gscv = GridSearchCV(estimator=rf,\n",
    "    #                     param_grid=params,\n",
    "    #                     n_jobs=-1,\n",
    "    #                     scoring=acc_scorer,\n",
    "    #                     cv=10)\n",
    "\n",
    "    gscv = GridSearchCV(estimator=gb,\n",
    "                    param_grid=gb_params,\n",
    "                    n_jobs=-1,\n",
    "                    scoring=acc_scorer,\n",
    "                    cv=10)\n",
    "    \n",
    "    # gscv = GridSearchCV(estimator=lr,\n",
    "    #             param_grid=lr_params,\n",
    "    #             n_jobs=-1,\n",
    "    #             scoring=acc_scorer,\n",
    "    #             cv=10)\n",
    "\n",
    "    clf = gscv.fit(df, y)\n",
    "    \n",
    "    model = clf.best_estimator_\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.33, random_state=66)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print(confusion_matrix(y_test, predictions))\n",
    "    \n",
    "    '''\n",
    "    print('Best parameters: {}'.format(clf.best_params_))\n",
    "    print('Best RMSLE: {}'.format(clf.best_score_))\n",
    "\n",
    "    test = pd.read_csv('data/test.csv')\n",
    "    test = test.sort_values(by='SalesID')\n",
    "\n",
    "    test_predictions = clf.predict(test)\n",
    "    test['SalePrice'] = test_predictions\n",
    "    outfile = 'data/solution_benchmark.csv'\n",
    "    test[['SalesID', 'SalePrice']].to_csv(outfile, index=False)\n",
    "    '''\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
